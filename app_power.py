# -*- coding: utf-8 -*-
"""
Q-ì •ë ¬ í˜„ì¥ ë¶„ì„ ì•± (Optimized for Stability & Speed)
- ì£¼ìš” ê°œì„ : Caching ì ìš©, ì˜ˆì™¸ ì²˜ë¦¬ ê°•í™”, ìˆ˜ì¹˜ í•´ì„ ì•ˆì •ì„± í™•ë³´
"""

import io
import re
import numpy as np
import pandas as pd
import streamlit as st
import matplotlib.pyplot as plt
from numpy.linalg import norm
from sklearn.decomposition import PCA
from scipy.linalg import orthogonal_procrustes
from scipy.stats import norm as zdist

# ========================= ì„¤ì • ë° ìƒìˆ˜ =========================
st.set_page_config(page_title="Q-Method Field Analysis", layout="wide")

EMAIL_COL_CAND = ["email", "Email", "E-mail", "respondent", "id", "ID"]
MIN_N_FOR_ANALYSIS = 10  # ë¶„ì„ ê°€ëŠ¥ ìµœì†Œ ì¸ì› ì™„í™” (í…ŒìŠ¤íŠ¸ ìš©ì´ì„±)
TOPK_STATEMENTS = 5
RNG_SEED = 42
rng = np.random.default_rng(RNG_SEED)

# ========================= ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ =========================

def _coerce_numeric(df: pd.DataFrame) -> pd.DataFrame:
    """ëª¨ë“  ì—´ì„ ìˆ«ìë¡œ ë³€í™˜, ì˜¤ë¥˜ ë°œìƒ ì‹œ NaN ì²˜ë¦¬"""
    return df.apply(pd.to_numeric, errors='coerce')

def _looks_like_qcol(name: str) -> bool:
    """ë¬¸í•­ ì—´ì¸ì§€ ì‹ë³„ (ë©”íƒ€ë°ì´í„° ì»¬ëŸ¼ ì œì™¸)"""
    name_l = str(name).strip().lower()
    meta_cols = ["email", "respondent", "id", "time", "name", "timestamp", "date"]
    return not any(k in name_l for k in meta_cols)

def common_C35_columns(parts_dict):
    """ì„¸íŠ¸ A/B/Cì˜ ê³µí†µ ì—´ ì¤‘ C01~C35 íŒ¨í„´ ì¶”ì¶œ (ëŒ€ì†Œë¬¸ì ë¬´ì‹œ)"""
    # C01 ~ C35, c01 ~ c35 í—ˆìš©
    pat = re.compile(r"^C(0[1-9]|[12][0-9]|3[0-5])$", re.IGNORECASE)
    
    def get_cols(df):
        return {c for c in df.columns if pat.match(str(c).strip())}
    
    try:
        cols_sets = [get_cols(parts_dict[k]) for k in ["A", "B", "C"] if k in parts_dict]
        if not cols_sets: return []
        common = set.intersection(*cols_sets)
        return sorted(list(common))
    except:
        return []

@st.cache_data(show_spinner=False)
def load_excel_parts(file_bytes: bytes, sheet_names=("PARTA", "PARTB", "PARTC")):
    """
    ì—‘ì…€ ë¡œë”© ë° ì „ì²˜ë¦¬ (ìºì‹± ì ìš©ë¨)
    """
    xls = pd.ExcelFile(io.BytesIO(file_bytes))
    parts = {}
    
    # ì‹œíŠ¸ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
    available_sheets = xls.sheet_names
    target_sheets = [s for s in sheet_names if s in available_sheets]
    
    if not target_sheets:
        raise ValueError(f"ì§€ì •ëœ ì‹œíŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. (ë°œê²¬ëœ ì‹œíŠ¸: {available_sheets})")

    for sname in target_sheets:
        # í—¤ë” ìë™ ì¸ì‹
        raw = pd.read_excel(xls, sheet_name=sname)
        
        # Email ì»¬ëŸ¼ ì°¾ê¸°
        email_col = next((c for c in raw.columns if str(c).strip() in EMAIL_COL_CAND), None)
        
        # Email ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ì¸ë±ìŠ¤ë¥¼ IDë¡œ ì‚¬ìš©
        if email_col is None:
            raw["_generated_id"] = [f"ID_{i+1}" for i in range(len(raw))]
            email_col = "_generated_id"
            
        # ë¬¸í•­ ë°ì´í„° ì •ì œ
        q_cols = [c for c in raw.columns if c != email_col and _looks_like_qcol(c)]
        num_df = _coerce_numeric(raw[q_cols])
        
        # ìœ íš¨ ë°ì´í„° í•„í„°ë§ (ê°’ì´ 3ê°œ ì´ìƒ ìˆëŠ” ë¬¸í•­ë§Œ)
        valid_cols = [c for c in num_df.columns if num_df[c].notna().sum() >= 3]
        
        df_final = num_df[valid_cols].copy()
        df_final.insert(0, "email", raw[email_col].fillna("Unknown").astype(str))
        
        # Keyë¥¼ A, B, Cë¡œ ë§¤í•‘ (PARTA -> A)
        key = sname.replace("PART", "")
        parts[key] = df_final.reset_index(drop=True)
        
    return parts

def ensure_q_columns(df: pd.DataFrame):
    """emailê³¼ ìˆ«ìí˜• ë¬¸í•­ ë¶„ë¦¬"""
    if df.empty:
        return df, ([], [])
        
    # ì²« ì»¬ëŸ¼ì„ emailë¡œ ê°€ì • (load_excel_partsì—ì„œ ì²˜ë¦¬ë¨)
    email_col = df.columns[0]
    dfn = df.select_dtypes(include=[np.number])
    Q_COLS = list(dfn.columns)
    
    df_out = pd.concat([df[[email_col]], dfn], axis=1)
    return df_out, (Q_COLS, [str(c) for c in Q_COLS])

# ========================= í†µê³„ ë° ë¶„ì„ ì½”ì–´ =========================

def standardize_rows(X: np.ndarray):
    """í–‰(ì‚¬ëŒ)ë³„ í‘œì¤€í™” (Z-score)"""
    # ddof=1 for sample std
    std = X.std(axis=1, ddof=1, keepdims=True)
    # í‘œì¤€í¸ì°¨ 0ì¸ ê²½ìš° 1ë¡œ ëŒ€ì²´í•˜ì—¬ ë‚˜ëˆ—ì…ˆ ì˜¤ë¥˜ ë°©ì§€
    std[std == 0] = 1.0 
    return (X - X.mean(axis=1, keepdims=True)) / std

def varimax(Phi, gamma=1.0, q=20, tol=1e-6):
    """Varimax Rotation (Numpy Implementation)"""
    p, k = Phi.shape
    R = np.eye(k)
    d = 0
    for i in range(q):
        d_old = d
        Lambda = np.dot(Phi, R)
        # SVD ì•ˆì •ì„± í™•ë³´
        u, s, vh = np.linalg.svd(
            np.dot(Phi.T, (Lambda**3 - (gamma/p) * np.dot(Lambda, np.diag(np.diag(np.dot(Lambda.T, Lambda))))))
        )
        R = np.dot(u, vh)
        d = np.sum(s)
        if d_old != 0 and d/d_old < 1 + tol:
            break
    return np.dot(Phi, R)

@st.cache_data(show_spinner=False)
def calculate_person_correlation(data_values: np.ndarray, metric="Pearson"):
    """
    ìƒê´€ê³„ìˆ˜ ê³„ì‚° (ìºì‹±ì„ ìœ„í•´ numpy array ì…ë ¥ ë°›ìŒ)
    Input: (n_persons, n_items)
    """
    if metric.lower().startswith("spear"):
        # Rank ë³€í™˜ (í–‰ ë³„ë¡œ)
        data_rank = np.apply_along_axis(lambda v: pd.Series(v).rank(method="average").to_numpy(), 1, data_values)
        data_norm = standardize_rows(data_rank)
    else:
        data_norm = standardize_rows(data_values)
    
    # Q-method: ì‚¬ëŒ ê°„ì˜ ìƒê´€ê³„ìˆ˜ (Rows=Persons)
    # np.corrcoefëŠ” í–‰(row)ì„ ë³€ìˆ˜ë¡œ ì¸ì‹í•˜ë¯€ë¡œ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    return np.corrcoef(data_norm)

def person_q_analysis(df_q: pd.DataFrame, corr_metric="Pearson", n_factors=None, rotate=True):
    """Q-Methodology Factor Analysis Pipeline"""
    # ë°ì´í„° ì¤€ë¹„
    df_only = df_q.drop(columns=["email"], errors="ignore")
    X = df_only.select_dtypes(include=[np.number]).to_numpy(dtype=float)
    
    # 1. ìƒê´€ê³„ìˆ˜ í–‰ë ¬ (R)
    R = calculate_person_correlation(X, metric=corr_metric)
    
    # 2. ê³ ìœ ê°’ ë¶„í•´ (Eigendecomposition)
    eigvals, eigvecs = np.linalg.eigh(R)
    # ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
    idx = eigvals.argsort()[::-1]
    eigvals = eigvals[idx]
    eigvecs = eigvecs[:, idx]
    
    # 3. ìš”ì¸ ìˆ˜ ê²°ì •
    if not n_factors or n_factors <= 0:
        n_factors = int(np.sum(eigvals > 1.0)) # Kaiser Rule
        n_factors = max(2, min(7, n_factors))  # Safety bounds
        
    # 4. ì ì¬ì¹˜ (Loadings) ì¶”ì¶œ (Centroid/PCA approach approximation)
    # loadings = eigenvector * sqrt(eigenvalue)
    loadings = eigvecs[:, :n_factors] * np.sqrt(eigvals[:n_factors])
    
    # 5. íšŒì „ (Rotation)
    if rotate:
        loadings = varimax(loadings)
        
    # 6. ìš”ì¸ ì ìˆ˜ (Factor Scores / Arrays) ê³„ì‚°
    # ë¬¸í•­ë³„ Z-score (Standardized items across people? No, Q uses weighted average of pure sorts)
    # ì—¬ê¸°ì„œëŠ” ê·¼ì‚¬ì ìœ¼ë¡œ ê°€ì¤‘ì¹˜ í•©ì‚° ë°©ì‹ ì‚¬ìš©
    
    # ë¬¸í•­ í‘œì¤€í™” (ì—´ ë°©í–¥)
    item_std = X.std(axis=0, ddof=1)
    item_std[item_std==0] = 1.0
    Z_items = (X - X.mean(axis=0)) / item_std
    
    arrays = []
    for j in range(n_factors):
        w = loadings[:, j]
        # ìš”ì¸ ì •ì˜ì— ê¸°ì—¬í•˜ëŠ” ì£¼ìš” ì‘ë‹µì ê°€ì¤‘ì¹˜ (Flagging logic ê°„ì†Œí™”)
        # Factor Loadingì˜ ì œê³±ì„ ê°€ì¤‘ì¹˜ë¡œ ì‚¬ìš©í•˜ê±°ë‚˜, ë‹¨ìˆœíˆ Loadingì„ ê°€ì¤‘ì¹˜ë¡œ ì‚¬ìš©
        # Q-method í‘œì¤€: z_factor = sum(loading * z_person) / sqrt(sum(loading^2)) 
        # ë³¸ ì½”ë“œëŠ” ê¸°ì¡´ ë¡œì§(Top respondent weighted avg) ìœ ì§€í•˜ë˜ ì•ˆì •ì„± ë³´ê°•
        
        weight_sum = np.sum(np.abs(w)) + 1e-9
        z_j = np.dot(Z_items.T, w) / weight_sum # Simple weighted average
        
        # Z-score normalization of the factor array itself
        z_j_std = z_j.std(ddof=1)
        if z_j_std == 0: z_j_std = 1.0
        z_j = (z_j - z_j.mean()) / z_j_std
        
        arrays.append(z_j)
        
    arrays = np.array(arrays) # (Factors x Items)
    
    return loadings, eigvals, R, arrays

def assign_types(loadings: np.ndarray, emails: list, thr=0.40, sep=0.10):
    """ì°¸ê°€ì ìš”ì¸ ë°°ì • ë¡œì§"""
    n_persons, n_factors = loadings.shape
    
    abs_loadings = np.abs(loadings)
    max_idx = abs_loadings.argmax(axis=1)
    max_val = abs_loadings.max(axis=1)
    
    # 2ë²ˆì§¸ë¡œ í° ê°’ ì°¾ê¸° (Gap ê³„ì‚°ìš©)
    sorted_vals = np.sort(abs_loadings, axis=1)[:, ::-1]
    second_val = sorted_vals[:, 1] if n_factors > 1 else np.zeros(n_persons)
    
    gap = max_val - second_val
    
    # ë°°ì • ì¡°ê±´: ìµœëŒ€ê°’ì´ ì„ê³„ì¹˜ ì´ìƒ AND ì°¨ì´ê°€ sep ì´ìƒ
    assigned = (max_val >= thr) & (gap >= sep)
    
    # Type ë¬¸ìì—´
    types = [f"Type{i+1}" if assign else "None" for i, assign in zip(max_idx, assigned)]
    
    return pd.DataFrame({
        "email": emails,
        "Type": types,
        "MaxLoading": loadings[np.arange(n_persons), max_idx], # ë¶€í˜¸ í¬í•¨ ì›ë˜ ê°’
        "AbsMax": max_val,
        "Gap": gap,
        "Assigned": assigned
    })

# ========================= êµì°¨ ë¶„ì„ í•¨ìˆ˜ (Caching) =========================

@st.cache_data(show_spinner=False)
def run_scree_parallel(df_values: np.ndarray, n_perm=300):
    """Scree Plot & Parallel Analysis"""
    n_persons, n_items = df_values.shape
    
    # ê´€ì¸¡ëœ ê³ ìœ ê°’
    R = np.corrcoef(standardize_rows(df_values))
    obs_eigs = np.linalg.eigvalsh(R)[::-1] # ë‚´ë¦¼ì°¨ìˆœ
    obs_eigs = np.maximum(obs_eigs, 0) # ìˆ˜ì¹˜ì  ì˜¤ì°¨ë¡œ ì¸í•œ ìŒìˆ˜ ì œê±°
    
    # ë¬´ì‘ìœ„ ìˆœì—´/ë…¸ì´ì¦ˆ ì‹œë®¬ë ˆì´ì…˜
    perm_eigs = np.zeros((n_perm, n_persons))
    
    for b in range(n_perm):
        # Random Normal Noise approach for Parallel Analysis
        noise = rng.standard_normal(size=(n_persons, n_items))
        R_noise = np.corrcoef(standardize_rows(noise))
        eigs_b = np.linalg.eigvalsh(R_noise)[::-1]
        perm_eigs[b] = eigs_b
        
    mean_perm = perm_eigs.mean(axis=0)
    p95_perm = np.percentile(perm_eigs, 95, axis=0)
    
    # k_star: ê´€ì¸¡ê°’ì´ ë¬´ì‘ìœ„ í‰ê· ë³´ë‹¤ í° ê°œìˆ˜
    k_star = int(np.sum(obs_eigs > mean_perm))
    
    return obs_eigs, mean_perm, p95_perm, k_star

def procrustes_congruence(LA, LB):
    """ìš”ì¸ êµ¬ì¡° ì¼ì¹˜ë„ (Tucker's Congruence Coefficient after Procrustes)"""
    # Procrustes Rotation: LBë¥¼ LAì— ë§ì¶¤
    R, _ = orthogonal_procrustes(LB, LA)
    LB_aligned = np.dot(LB, R)
    
    # Congruence Coefficient (Cosines)
    phis = []
    for j in range(LA.shape[1]):
        num = np.dot(LA[:, j], LB_aligned[:, j])
        den = norm(LA[:, j]) * norm(LB_aligned[:, j]) + 1e-9
        phis.append(float(num / den))
        
    return np.array(phis)

@st.cache_data(show_spinner=False)
def bootstrap_factor_stability(data_values: np.ndarray, k=5, B=500, phi_threshold=0.80):
    """ë¶€íŠ¸ìŠ¤íŠ¸ë© ì•ˆì •ë„ ê²€ì¦ (ìƒë‹¹í•œ ì—°ì‚°ëŸ‰ -> ìºì‹± í•„ìˆ˜)"""
    N = data_values.shape[0]
    
    # Base Solution
    pca = PCA(n_components=k, random_state=RNG_SEED)
    base_L = pca.fit_transform(standardize_rows(data_values))
    
    phis = []
    for b in range(B):
        # Resample with replacement
        idx = rng.choice(N, size=N, replace=True)
        sample = data_values[idx]
        
        # 3ëª… ë¯¸ë§Œì´ë©´ PCA ë¶ˆê°€
        if len(np.unique(idx)) < 3: continue
            
        pca_b = PCA(n_components=k, random_state=None)
        Lb = pca_b.fit_transform(standardize_rows(sample))
        
        # Procrustes & Congruence
        phi_vals = procrustes_congruence(base_L, Lb)
        phis.append(phi_vals)
        
    if not phis:
        return None
        
    PHI = np.array(phis)
    return {
        'phi_mean': PHI.mean(axis=0),
        'phi_std': PHI.std(axis=0),
        'stability_rate': (PHI >= phi_threshold).mean(axis=0)
    }

# ========================= UI ë ˆì´ì•„ì›ƒ =========================

st.title("ğŸ“Š Q-Method Field Analysis")
st.caption("Robust & Optimized Ver. | PART A/B/C Integration")

with st.sidebar:
    st.header("1. Data Upload")
    file = st.file_uploader("Upload Excel (Sheets: PARTA, PARTB, PARTC)", type=["xlsx"])
    
    if file:
        try:
            parts = load_excel_parts(file.getvalue())
            st.success(f"Loaded: {', '.join(parts.keys())}")
        except Exception as e:
            st.error(f"Error: {e}")
            st.stop()
    else:
        st.info("íŒŒì¼ì´ ì—†ë‹¤ë©´ ì½”ë“œë¥¼ í™•ì¸í•˜ê±°ë‚˜ ì˜ˆì œ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.")
        st.stop()

tab_list = ["Set A", "Set B", "Set C", "Cross-Analysis", "Distinguishing", "Stability"]
tabs = st.tabs(tab_list)

# ---------- ì„¸íŠ¸ë³„ ë¶„ì„ (A, B, C) ----------
def run_set_tab(tab_obj, df_set, set_name):
    with tab_obj:
        st.markdown(f"### {set_name} Analysis")
        
        df_q, (cols, col_names) = ensure_q_columns(df_set)
        
        # ê²°ì¸¡ì¹˜ ë§ì€ ì‘ë‹µì ì œê±° (60% ì´ìƒ ì‘ë‹µ í•„ìˆ˜)
        valid_mask = df_q[cols].notna().sum(axis=1) >= (len(cols) * 0.6)
        df_clean = df_q[valid_mask].copy()
        
        n_obs = len(df_clean)
        st.markdown(f"**N = {n_obs}** (Valid Respondents) | Items = {len(cols)}")
        
        if n_obs < MIN_N_FOR_ANALYSIS:
            st.warning(f"ë°ì´í„°ê°€ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤. (ìµœì†Œ {MIN_N_FOR_ANALYSIS}ëª… ê¶Œì¥)")
            return

        # ì˜µì…˜
        c1, c2, c3 = st.columns(3)
        with c1:
            metric = st.selectbox("Correlation", ["Pearson", "Spearman"], key=f"corr_{set_name}")
        with c2:
            n_factors = st.number_input("Factors (0=Auto)", 0, 7, 0, key=f"nf_{set_name}")
        with c3:
            rotate = st.checkbox("Varimax Rotation", True, key=f"rot_{set_name}")
            
        # ë¶„ì„ ì‹¤í–‰
        try:
            loadings, eigvals, R, arrays = person_q_analysis(
                df_clean, metric, n_factors, rotate
            )
            
            K = loadings.shape[1]
            factor_cols = [f"F{i+1}" for i in range(K)]
            
            # 1. Loadings Table
            st.markdown("#### Factor Loadings")
            load_df = pd.DataFrame(loadings, columns=factor_cols)
            load_df.insert(0, "email", df_clean["email"].values)
            
            # ìŠ¤íƒ€ì¼ë§: ë†’ì€ ì ì¬ì¹˜ ê°•ì¡°
            st.dataframe(
                load_df.style.background_gradient(cmap="Blues", subset=factor_cols, vmin=-1, vmax=1),
                use_container_width=True
            )
            
            # 2. Type Assignment
            assign_df = assign_types(loadings, df_clean["email"].values)
            st.markdown("#### Type Assignment Summary")
            counts = assign_df[assign_df["Assigned"]]["Type"].value_counts().sort_index()
            st.write(counts.to_dict())
            
            with st.expander("See Assignment Details"):
                st.dataframe(assign_df)
                
            # 3. Factor Arrays (Z-scores)
            st.markdown("#### Factor Arrays (Item Z-scores)")
            arrays_df = pd.DataFrame(arrays.T, index=col_names, columns=factor_cols)
            st.dataframe(arrays_df.style.background_gradient(cmap="RdBu_r", vmin=-2, vmax=2), use_container_width=True)
            
            # Download
            csv = arrays_df.to_csv().encode('utf-8-sig')
            st.download_button("ğŸ“¥ Download Factor Arrays", csv, f"{set_name}_arrays.csv", "text/csv")
            
        except Exception as e:
            st.error(f"Analysis Error: {str(e)}")

# íƒ­ ì‹¤í–‰
if "A" in parts: run_set_tab(tabs[0], parts["A"], "Set A")
else: tabs[0].info("No data for Set A")

if "B" in parts: run_set_tab(tabs[1], parts["B"], "Set B")
else: tabs[1].info("No data for Set B")

if "C" in parts: run_set_tab(tabs[2], parts["C"], "Set C")
else: tabs[2].info("No data for Set C")


# ---------- ê³µí†µ êµì°¨ë¶„ì„ ----------
with tabs[3]:
    st.header("Cross-Set Analysis (Common Items)")
    
    common_ids = common_C35_columns(parts)
    selected_common = st.multiselect("Select Common Items (C01~C35)", common_ids, default=common_ids)
    
    if len(selected_common) < 5:
        st.warning("ìµœì†Œ 5ê°œ ì´ìƒì˜ ê³µí†µ ë¬¸í•­ì´ í•„ìš”í•©ë‹ˆë‹¤.")
    else:
        col_res = st.columns(3)
        k_stars = []
        
        # 1. Scree Plots
        for idx, sname in enumerate(["A", "B", "C"]):
            if sname not in parts: continue
            
            with col_res[idx]:
                st.subheader(f"Set {sname}")
                df_part = parts[sname]
                # ê³µí†µ ë¬¸í•­ ë°ì´í„° ì¶”ì¶œ (ìˆ«ìí˜•)
                data_vals = df_part[selected_common].apply(pd.to_numeric, errors='coerce').fillna(0).values
                
                if data_vals.shape[0] < 5:
                    st.info("Not enough data")
                    continue
                    
                obs, mean_p, _, k_star = run_scree_parallel(data_vals)
                k_stars.append(k_star)
                
                fig, ax = plt.subplots(figsize=(4, 3))
                ax.plot(range(1, len(obs)+1), obs, 'o-', label='Observed')
                ax.plot(range(1, len(mean_p)+1), mean_p, 'x--', label='Simulated', alpha=0.7)
                ax.axvline(k_star, color='red', linestyle=':', alpha=0.5)
                ax.set_title(f"Scree Plot (k*={k_star})")
                ax.legend(fontsize='small')
                st.pyplot(fig)
        
        # 2. Congruence
        if len(k_stars) >= 2:
            st.divider()
            rec_k = int(np.median(k_stars)) if k_stars else 3
            rec_k = max(2, min(5, rec_k))
            
            st.markdown(f"#### Factor Congruence (Recommended k={rec_k})")
            
            # ê° ì„¸íŠ¸ì˜ PCA Loadings ê³„ì‚°
            loadings_map = {}
            for sname in ["A", "B", "C"]:
                if sname in parts:
                    data = parts[sname][selected_common].apply(pd.to_numeric).fillna(0).values
                    # Item Loadings (Variables=Items for consistency check)
                    pca = PCA(n_components=rec_k)
                    # For item congruence, we usually look at item loadings
                    # Normalizing columns (items) standard approach
                    L = pca.fit_transform(data.T) # (Items x k)
                    loadings_map[sname] = L
            
            res_rows = []
            pairs = [('A','B'), ('A','C'), ('B','C')]
            for s1, s2 in pairs:
                if s1 in loadings_map and s2 in loadings_map:
                    phis = procrustes_congruence(loadings_map[s1], loadings_map[s2])
                    res_rows.append({
                        "Pair": f"{s1}-{s2}",
                        "Mean Phi": np.mean(phis),
                        "Min Phi": np.min(phis),
                        "Phi per Factor": [round(p,3) for p in phis]
                    })
            
            st.dataframe(pd.DataFrame(res_rows))

# ---------- êµ¬ë³„ ì§„ìˆ  ë¶„ì„ ----------
with tabs[4]:
    st.header("Distinguishing Statements")
    
    target_set = st.selectbox("Target Set", ["A", "B", "C"])
    use_common_only = st.checkbox("Use Common Items Only (C01~C35)", value=True)
    
    if target_set in parts:
        df_curr = parts[target_set]
        
        if use_common_only:
            cols = common_C35_columns(parts)
        else:
            _, (cols, _) = ensure_q_columns(df_curr)
            
        if not cols:
            st.error("No columns found.")
        else:
            data = df_curr[cols].apply(pd.to_numeric, errors='coerce').fillna(0)
            
            # Quick Factor Analysis for Arrays
            k_dist = st.number_input("Number of Factors", 2, 6, 3, key="dist_k")
            
            # Logic:
            # 1. Calculate Factor Arrays (Z-scores)
            # 2. Calculate Difference between Factors
            # 3. Significance Test
            
            try:
                # Reuse core logic manually for custom columns
                # Person Correlation
                R = np.corrcoef(standardize_rows(data.values))
                eigvals, eigvecs = np.linalg.eigh(R)
                idx = eigvals.argsort()[::-1]
                L = eigvecs[:, idx][:, :k_dist] * np.sqrt(eigvals[idx][:k_dist])
                L = varimax(L)
                
                # Arrays
                Z_items = (data - data.mean()) / (data.std() + 1e-9)
                arrays_list = []
                for j in range(k_dist):
                    w = L[:, j]
                    z_j = np.dot(Z_items.T, w) / (np.sum(np.abs(w)) + 1e-9)
                    arrays_list.append(z_j)
                
                Z_arr = pd.DataFrame(np.array(arrays_list).T, index=cols, columns=[f"F{i+1}" for i in range(k_dist)])
                
                st.subheader("Significant Differences (p < .01)")
                
                diff_rows = []
                SE = 1.0 / np.sqrt(len(data)) # Standard Error approx
                
                for item in Z_arr.index:
                    vals = Z_arr.loc[item].values
                    for i in range(k_dist):
                        for j in range(i+1, k_dist):
                            diff = vals[i] - vals[j]
                            z_score = diff / (np.sqrt(2) * SE * 1.96) # Simplified Z
                            # Simple logic: if difference > threshold
                            # Precise Q-method: diff > 2.58 * (SE * sqrt(2)) for p<.01
                            crit_val = 2.58 * (SE * np.sqrt(2))
                            
                            if abs(diff) > crit_val:
                                diff_rows.append({
                                    "Item": item,
                                    "F_High": f"F{i+1}" if diff > 0 else f"F{j+1}",
                                    "F_Low": f"F{j+1}" if diff > 0 else f"F{i+1}",
                                    "Diff": abs(diff),
                                    "Critical_Val_01": crit_val
                                })
                                
                if diff_rows:
                    st.dataframe(pd.DataFrame(diff_rows).sort_values("Diff", ascending=False), use_container_width=True)
                else:
                    st.info("No distinguishing statements found at p < .01")
                    
            except Exception as e:
                st.error(f"Calculation Error: {e}")

# ---------- ì•ˆì •ë„ (Bootstrap) ----------
with tabs[5]:
    st.header("Bootstrap Stability")
    st.caption("Resampling Items/Persons to check factor stability")
    
    col_b1, col_b2 = st.columns(2)
    with col_b1:
        bs_set = st.selectbox("Set for Bootstrap", ["A", "B", "C"])
    with col_b2:
        n_iter = st.number_input("Iterations (B)", 100, 1000, 200, step=100)
        
    if bs_set in parts:
        cols = common_C35_columns(parts)
        if len(cols) < 5:
            st.error("Need more common items.")
        else:
            data_bs = parts[bs_set][cols].apply(pd.to_numeric, errors='coerce').fillna(0).values
            
            if st.button("Run Bootstrap"):
                with st.spinner("Running Bootstrap..."):
                    res = bootstrap_factor_stability(data_bs, k=3, B=n_iter)
                    
                if res:
                    st.markdown("### Stability Results")
                    df_res = pd.DataFrame({
                        "Mean Phi": res['phi_mean'],
                        "Std Phi": res['phi_std'],
                        "Stable Rate (>0.80)": res['stability_rate']
                    }, index=[f"F{i+1}" for i in range(len(res['phi_mean']))])
                    
                    st.dataframe(df_res.style.background_gradient(cmap="Greens", subset=["Stable Rate (>0.80)"]))
                    
                    st.info("í•´ì„: Stable Rateê°€ 0.8 ì´ìƒì¸ ìš”ì¸ì€ í‘œë³¸ ë³€ë™ì—ë„ ê²¬ê³ í•œ ìš”ì¸ìœ¼ë¡œ ê°„ì£¼ë©ë‹ˆë‹¤.")
                else:
                    st.error("Bootstrap Failed (Data issue)")
