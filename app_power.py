"""
Q-Method (TADT Research) â€” Q Analyzer
@Author: Prof. Dr. Songhee Kang
@Date: 2025.08.14. 
Q-ì •ë ¬ í˜„ì¥ ë¶„ì„ ì•± (PARTA/PARTB/PARTC)
- ì„¸íŠ¸ë³„(Q-ì •ë ¬) : ìš”ì¸ ì¶”ì¶œ/ì‚¬ëŒ-ìš”ì¸ ì ì¬/ìœ í˜• ë°°ì •/ìƒí•˜ìœ„ ì§„ìˆ 
- ê³µí†µë¬¸í•­ êµì°¨ë¶„ì„ : ìŠ¤í¬ë¦¬+ë³‘ë ¬, Procrustes(ì„¸íŠ¸ ê°„ ì¼ì¹˜ë„), ì„¤ëª…ë¶„ì‚°
- êµ¬ë³„ì§„ìˆ  : z-array ê·¼ì‚¬, z-ì°¨ ìœ ì˜ì„±, Humphreyâ€™s rule
- ë¶€íŠ¸ìŠ¤íŠ¸ë© : ê³µí†µë¬¸í•­ ìš”ì¸ ì•ˆì •ë„(Ï† ì„ê³„ ì´ìƒ ë¹„ìœ¨)

í•„ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬: pandas, numpy, scipy, scikit-learn, matplotlib, openpyxl, streamlit
"""
import os, io, re
import numpy as np
import pandas as pd
import streamlit as st
import matplotlib.pyplot as plt
from numpy.linalg import norm
from sklearn.decomposition import PCA
from scipy.linalg import orthogonal_procrustes
from scipy.stats import norm as zdist

st.set_page_config(page_title="Q-ì •ë ¬ í˜„ì¥ ë¶„ì„", layout="wide")

# ========================= ê³µí†µ ìœ í‹¸/í•µì‹¬ í•¨ìˆ˜ =========================
EMAIL_COL_CAND = ["email", "Email", "E-mail", "respondent", "id"]
MIN_N_FOR_ANALYSIS = 20
TOPK_STATEMENTS = 5
RNG_SEED = 42
rng = np.random.default_rng(RNG_SEED)

def _coerce_numeric(df):
    out = df.copy()
    for c in out.columns:
        out[c] = pd.to_numeric(out[c], errors="coerce")
    return out

def _looks_like_qcol(name: str):
    name_l = str(name).strip().lower()
    if any(k in name_l for k in ["email","respondent","id","time","name","timestamp"]):
        return False
    return True

@st.cache_data(show_spinner=False)
def load_excel_parts(file_bytes: bytes, sheet_names=("PARTA","PARTB","PARTC")):
    """ì—…ë¡œë“œëœ ì—‘ì…€ ë°”ì´ë„ˆë¦¬ì—ì„œ PARTA/B/C ì‹œíŠ¸ ë¡œë“œ â†’ dict[set_id]=DataFrame(email+ë¬¸í•­)"""
    xls = pd.ExcelFile(io.BytesIO(file_bytes))
    parts = {}
    for sid, sname in zip(["A","B","C"], sheet_names):
        if sname not in xls.sheet_names:
            raise ValueError(f"ì‹œíŠ¸ '{sname}' ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì—‘ì…€ ì‹œíŠ¸: {xls.sheet_names}")
        raw = pd.read_excel(xls, sheet_name=sname)
        # email/ID
        email_col = None
        for c in raw.columns:
            if str(c).strip().lower() in [e.lower() for e in EMAIL_COL_CAND]:
                email_col = c; break
        if email_col is None:
            raw["email"] = ""
            email_col = "email"
        # ë¬¸í•­ì—´ í›„ë³´
        q_cols = [c for c in raw.columns if c!=email_col and _looks_like_qcol(c)]
        num = _coerce_numeric(raw[q_cols])
        valid_cols = [c for c in num.columns if num[c].notna().sum()>=3]  # ìµœì†Œ ì‘ë‹µ 3
        df_q = num[valid_cols].copy()
        df_q.insert(0, "email", raw[email_col].fillna("").astype(str))
        parts[sid] = df_q.reset_index(drop=True)
    return parts

def ensure_q_columns(df: pd.DataFrame, q_count=None):
    """email + ë¬¸í•­ì—´ ë°˜í™˜, Q_COLS/Q_SET ì œê³µ"""
    cols = list(df.columns)
    email_col = "email" if cols and str(cols[0]).lower()=="email" else None
    if email_col is None:
        df = df.copy()
        df.insert(0, "email", "")
    Q_COLS = [c for c in df.columns if c!="email"]
    if q_count and len(Q_COLS)>q_count:
        Q_COLS = Q_COLS[:q_count]
    Q_SET = [str(c) for c in Q_COLS]
    return df[["email"]+Q_COLS], (Q_COLS, Q_SET)

def standardize_people_rows(X: np.ndarray):
    return (X - X.mean(axis=1, keepdims=True)) / (X.std(axis=1, ddof=1, keepdims=True)+1e-8)

def person_correlation(df_only: pd.DataFrame, metric="Pearson"):
    X = df_only.to_numpy(dtype=float)
    if metric.lower().startswith("spear"):
        X_rank = np.apply_along_axis(lambda v: pd.Series(v).rank(method="average").to_numpy(), 0, X)
        Xs = standardize_people_rows(X_rank)
    else:
        Xs = standardize_people_rows(X)
    return np.corrcoef(Xs)

def varimax(Phi, gamma=1.0, q=60, tol=1e-6):
    from numpy import eye, dot
    p,k = Phi.shape
    R = eye(k); d=0
    for i in range(q):
        d_old = d
        Lambda = dot(Phi, R)
        u,s,vh = np.linalg.svd(dot(Phi.T, (Lambda**3 - (gamma/p)*dot(Lambda, np.diag(np.diag(dot(Lambda.T,Lambda)))))))
        R = dot(u, vh); d = s.sum()
        if d_old!=0 and d/d_old < 1+tol: break
    return dot(Phi, R)

def person_q_analysis(df_q: pd.DataFrame, corr_metric="Pearson", n_factors=None, rotate=True):
    df_only = df_q.drop(columns=["email"], errors="ignore")
    R = person_correlation(df_only, metric=corr_metric)
    eigvals, eigvecs = np.linalg.eigh(R)
    idx = eigvals.argsort()[::-1]; eigvals = eigvals[idx]; eigvecs = eigvecs[:,idx]
    # ìë™ ìš”ì¸ìˆ˜(ê³ ìœ ê°’>1) 2~6 ì œí•œ
    if not n_factors or n_factors<=0:
        n_factors = int(np.sum(eigvals > 1.0))
        n_factors = max(2, min(6, n_factors))
    loadings = eigvecs[:, :n_factors]*np.sqrt(eigvals[:n_factors])  # ì‚¬ëŒÃ—ìš”ì¸
    # z-array ê·¼ì‚¬
    X = df_only.to_numpy(dtype=float)
    Z_items = (X - X.mean(axis=0)) / (X.std(axis=0, ddof=1)+1e-8)  # ì‚¬ëŒÃ—ë¬¸í•­
    arrays = []
    for j in range(n_factors):
        w = loadings[:,j]
        idx_top = np.argsort(np.abs(w))[::-1][:max(5, int(0.1*len(w)))]
        z_j = (Z_items[idx_top].T @ w[idx_top]) / (np.sum(np.abs(w[idx_top])) + 1e-8)
        arrays.append(z_j)
    arrays = np.array(arrays)
    if rotate:
        arrays = varimax(arrays.T).T  # ë¬¸í•­Ã—ìš”ì¸ â†’ íšŒì „ â†’ ìš”ì¸Ã—ë¬¸í•­
    return loadings, eigvals, R, arrays  # arrays: TypeÃ—Q

def assign_types(loadings: np.ndarray, emails: list, thr=0.40, sep=0.10):
    K = loadings.shape[1]
    max_idx = loadings.argmax(axis=1)
    max_val = loadings.max(axis=1)
    sorted_vals = np.sort(np.abs(loadings), axis=1)[:, ::-1]
    gap = sorted_vals[:,0] - sorted_vals[:,1]
    assigned = (max_val>=thr) & (gap>=sep)
    return pd.DataFrame({
        "email": emails,
        "Type": [f"Type{int(i)+1}" for i in max_idx],
        "MaxLoading": max_val,
        "Gap": gap,
        "Assigned": assigned
    })

def top_bottom_statements(arrays: np.ndarray, topk=TOPK_STATEMENTS):
    tb = []
    for t in range(arrays.shape[0]):
        z = arrays[t]
        top_idx = np.argsort(z)[::-1][:topk]
        bot_idx = np.argsort(z)[:topk]
        tb.append((top_idx, bot_idx, z))
    return tb

# ===== ê³µí†µë¬¸í•­: ìŠ¤í¬ë¦¬+ë³‘ë ¬, Procrustes, z-array/êµ¬ë³„, ë¶€íŠ¸ìŠ¤íŠ¸ë© =====
def scree_and_parallel(df, n_perm=500, show_plot=True):
    R = person_correlation(df)
    eigvals = np.linalg.eigvalsh(R)[::-1]
    p = R.shape[0]
    perm_eigs = np.zeros((n_perm, p))
    for b in range(n_perm):
        X = rng.standard_normal(size=df.shape)
        X = (X - X.mean(axis=1, keepdims=True))/(X.std(axis=1, ddof=1, keepdims=True)+1e-8)
        Rb = np.corrcoef(X); perm_eigs[b] = np.linalg.eigvalsh(Rb)[::-1]
    mean_perm = perm_eigs.mean(axis=0)
    k_star = int(np.sum(eigvals > mean_perm))
    fig = None
    if show_plot:
        fig, ax = plt.subplots(figsize=(6,4))
        ax.plot(range(1,p+1), eigvals, marker='o', label='Observed')
        ax.plot(range(1,p+1), mean_perm, marker='x', label='Parallel mean')
        ax.axvline(k_star, color='r', linestyle='--', label=f'k*={k_star}')
        ax.set_xlabel('Factor number'); ax.set_ylabel('Eigenvalue'); ax.set_title('Scree + Parallel')
        ax.legend(); fig.tight_layout()
    return {'eigvals': eigvals, 'parallel_mean': mean_perm, 'k_star': k_star, 'fig': fig}

def pca_loadings_on_items(df, k=5):
    X = df.to_numpy(dtype=float)
    X = (X - X.mean(axis=0, keepdims=True))/(X.std(axis=0, ddof=1, keepdims=True)+1e-8)
    pca = PCA(n_components=k, random_state=RNG_SEED).fit(X)
    L = pca.components_.T
    for j in range(L.shape[1]): L[:,j] /= (norm(L[:,j])+1e-8)
    return L, pca.explained_variance_ratio_.sum()

def procrustes_congruence(LA, LB):
    R, _ = orthogonal_procrustes(LB, LA)
    LB_aligned = LB @ R
    phis = []
    for j in range(LA.shape[1]):
        a, b = LA[:,j], LB_aligned[:,j]
        phis.append(float((a@b)/(norm(a)*norm(b)+1e-8)))
    return np.array(phis)

def congruence_across_sets(dfA, dfB, dfC, common_ids, k=5):
    A = dfA[common_ids].copy(); B = dfB[common_ids].copy(); C = dfC[common_ids].copy()
    LA, varA = pca_loadings_on_items(A, k); LB, varB = pca_loadings_on_items(B, k); LC, varC = pca_loadings_on_items(C, k)
    phi_AB = procrustes_congruence(LA, LB); phi_AC = procrustes_congruence(LA, LC); phi_BC = procrustes_congruence(LB, LC)
    return {'phi_mean_AB': float(np.mean(phi_AB)), 'phi_mean_AC': float(np.mean(phi_AC)), 'phi_mean_BC': float(np.mean(phi_BC)),
            'phi_AB': phi_AB, 'phi_AC': phi_AC, 'phi_BC': phi_BC, 'explained_var': {'A':varA,'B':varB,'C':varC}}

def q_factor_solution(df, k=5):
    X = df.to_numpy(dtype=float)
    Xs = (X - X.mean(axis=1, keepdims=True))/(X.std(axis=1, ddof=1, keepdims=True)+1e-8)
    R = np.corrcoef(Xs); pca = PCA(n_components=k, random_state=RNG_SEED).fit(R)
    Lp = pca.components_.T
    Z_items = (df - df.mean(axis=0))/(df.std(axis=0, ddof=1)+1e-8)
    z_arrays = []
    for j in range(k):
        w = Lp[:,j]; idx = np.argsort(np.abs(w))[::-1][:max(5, int(0.1*len(w)))]
        z_j = (Z_items.iloc[idx].T @ w[idx])/(np.sum(np.abs(w[idx]))+1e-8)
        z_arrays.append(z_j)
    Z = pd.DataFrame(np.column_stack(z_arrays), index=df.columns, columns=[f"F{t+1}" for t in range(k)])
    return Z, Lp

def distinguishing_tests(Z, alpha=0.01, se=0.30):
    items = Z.index; k = Z.shape[1]; rows=[]
    for itm in items:
        row = Z.loc[itm].values
        for a in range(k):
            for b in range(a+1,k):
                diff = row[a]-row[b]; z = diff/(np.sqrt(2)*se+1e-8); p = 2*(1-zdist.cdf(abs(z)))
                if p<alpha: rows.append((itm,f"F{a+1}",f"F{b+1}",diff,z,p))
    return pd.DataFrame(rows, columns=["item","F_high","F_low","z_diff","z_stat","p"]).sort_values("p")

def humphreys_rule(Lp):
    N = Lp.shape[0]; thr = 2*(1/np.sqrt(N)); flags={}
    for j in range(Lp.shape[1]):
        w = np.sort(np.abs(Lp[:,j]))[::-1][:2]
        flags[f"F{j+1}"] = bool(w[0]*w[1] > thr)
    return flags, thr

def bootstrap_factor_stability(df_common, k=5, B=500, phi_threshold=0.80):
    base_L, _ = pca_loadings_on_items(df_common, k); N = df_common.shape[0]; phis=[]
    for b in range(B):
        idx = rng.integers(low=0, high=N, size=N)
        Lb, _ = pca_loadings_on_items(df_common.iloc[idx], k)
        R, _ = orthogonal_procrustes(Lb, base_L); Lba = Lb @ R
        phis.append([float((base_L[:,j]@Lba[:,j])/(norm(base_L[:,j])*norm(Lba[:,j])+1e-8)) for j in range(k)])
    PHI = np.array(phis)
    return {'phi_mean': PHI.mean(axis=0), 'phi_std': PHI.std(axis=0), 'stability_rate': (PHI>=phi_threshold).mean(axis=0),
            'B':B,'phi_threshold':phi_threshold}

# ========================= ì‚¬ì´ë“œë°”: ë°ì´í„° ì—…ë¡œë“œ =========================
st.sidebar.header("ë°ì´í„° ì—…ë¡œë“œ")
file = st.sidebar.file_uploader("ì—‘ì…€ ì—…ë¡œë“œ (ì‹œíŠ¸: PARTA, PARTB, PARTC)", type=["xlsx"])
if file is None:
    st.info("ì—‘ì…€ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”. (ì˜ˆ: responses_power.xlsx.xlsx)")
    st.stop()

# ì‹œíŠ¸ ë¡œë“œ
try:
    parts = load_excel_parts(file.getvalue(), sheet_names=("PARTA","PARTB","PARTC"))
    st.sidebar.success("ì‹œíŠ¸ ë¡œë”© ì™„ë£Œ")
except Exception as e:
    st.sidebar.error(f"ì—‘ì…€ ë¡œë”© ì˜¤ë¥˜: {e}")
    st.stop()

# ========================= íƒ­ êµ¬ì„± =========================
tabA, tabB, tabC, tabCross, tabDist, tabBoot = st.tabs(["ì„¸íŠ¸ A", "ì„¸íŠ¸ B", "ì„¸íŠ¸ C", "ê³µí†µ êµì°¨ë¶„ì„", "êµ¬ë³„ì§„ìˆ ", "ë¶€íŠ¸ìŠ¤íŠ¸ë©"])

# ---------- ê³µí†µ í•¨ìˆ˜: íƒ­ ë¶„ì„ UI ----------
def run_set_tab(df_set, title="ì„¸íŠ¸"):
    st.subheader(f"{title} â€” ì‚¬ëŒ ìš”ì¸í™”(Q) ë¶„ì„")
    df_set, (Q_COLS, Q_SET) = ensure_q_columns(df_set, q_count=None)
    df_q = df_set[Q_COLS].copy()
    mask = df_q.notna().sum(axis=1) >= int(0.6*len(Q_COLS))
    df_q = df_q[mask]; emails = df_set.loc[mask,"email"].fillna("").astype(str).tolist()

    st.write(f"ìœ íš¨ ì‘ë‹µì ìˆ˜: **{len(df_q)}ëª…** / ë¬¸í•­ ìˆ˜: **{len(Q_COLS)}**")
    if len(df_q) < MIN_N_FOR_ANALYSIS:
        st.warning(f"ë¶„ì„ì— ìµœì†Œ {MIN_N_FOR_ANALYSIS}ëª…ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        return

    with st.expander("âš™ï¸ ë¶„ì„ ì˜µì…˜", expanded=True):
        colA, colB, colC = st.columns(3)
        with colA:
            corr_metric = st.selectbox("ìƒê´€ê³„ìˆ˜", ["Pearson","Spearman"], index=0, key=f"{title}_corr")
        with colB:
            n_f_override = st.number_input("ìš”ì¸ ìˆ˜(0=ìë™)", min_value=0, max_value=6, value=0, step=1, key=f"{title}_nf")
            n_factors = None if n_f_override==0 else int(n_f_override)
        with colC:
            rotate = st.checkbox("Varimax íšŒì „", value=True, key=f"{title}_rot")
        thr = st.slider("ìœ í˜• ë°°ì • ì„ê³„(ìµœëŒ€ ì ì¬)", 0.20, 0.70, 0.40, 0.05, key=f"{title}_thr")
        sep = st.slider("1ë“±-2ë“± ê²©ì°¨", 0.00, 0.50, 0.10, 0.05, key=f"{title}_sep")

    try:
        loadings, eigvals, R, arrays = person_q_analysis(pd.concat([df_set[["email"]], df_q], axis=1), corr_metric, n_factors, rotate)
        K = loadings.shape[1]
        st.markdown(f"**ì¶”ì¶œ ìš”ì¸ ìˆ˜: {K}**")
        load_df = pd.DataFrame(loadings, columns=[f"Type{i+1}" for i in range(K)])
        load_df.insert(0, "email", emails)
        st.dataframe(load_df.style.background_gradient(cmap="Blues", axis=None), use_container_width=True)

        assign_df = assign_types(loadings, emails, thr=thr, sep=sep)
        st.markdown("### ì°¸ê°€ì ìœ í˜• ë°°ì •")
        st.dataframe(assign_df, use_container_width=True)
        st.write("ìœ í˜•ë³„ ì¸ì›ìˆ˜:", assign_df[assign_df["Assigned"]].groupby("Type").size().to_dict())

        st.download_button("ğŸ“¥ ì°¸ê°€ì-ìœ í˜• ë°°ì • CSV",
                           data=assign_df.to_csv(index=False).encode("utf-8-sig"),
                           file_name=f"{title}_person_type_assignments.csv", mime="text/csv")

        arrays_df = pd.DataFrame(arrays, columns=Q_COLS, index=[f"Type{i+1}" for i in range(K)])
        st.markdown("### ìœ í˜•ë³„ factor array (ì§„ìˆ  z-í”„ë¡œíŒŒì¼)")
        st.dataframe(arrays_df, use_container_width=True)
        st.download_button("ğŸ“¥ ìœ í˜•ë³„ factor array CSV",
                           data=arrays_df.to_csv().encode("utf-8-sig"),
                           file_name=f"{title}_type_factor_arrays.csv", mime="text/csv")

        st.markdown(f"### ìœ í˜•ë³„ ìƒ/í•˜ìœ„ ì§„ìˆ  Top {TOPK_STATEMENTS}")
        tb = top_bottom_statements(arrays, topk=TOPK_STATEMENTS)
        for i, (top_idx, bot_idx, z) in enumerate(tb, start=1):
            with st.expander(f"Type{i} ìƒ/í•˜ìœ„ ì§„ìˆ ", expanded=(i==1)):
                st.markdown("**ìƒìœ„(+) ì§„ìˆ **")
                for j in top_idx:
                    st.write(f"- {Q_COLS[j]} (z={z[j]:.2f})")
                st.markdown("**í•˜ìœ„(âˆ’) ì§„ìˆ **")
                for j in bot_idx:
                    st.write(f"- {Q_COLS[j]} (z={z[j]:.2f})")
    except Exception as e:
        st.error(f"{title} ë¶„ì„ ì˜¤ë¥˜: {e}")

with tabA:
    run_set_tab(parts["A"], title="ì„¸íŠ¸ A")
with tabB:
    run_set_tab(parts["B"], title="ì„¸íŠ¸ B")
with tabC:
    run_set_tab(parts["C"], title="ì„¸íŠ¸ C")

# ---------- ê³µí†µë¬¸í•­ êµì°¨ë¶„ì„ ----------
with tabCross:
    st.subheader("ê³µí†µë¬¸í•­ êµì°¨ë¶„ì„ (Scree+Parallel, Procrustes, ì„¤ëª…ë¶„ì‚°)")
    # ê³µí†µë¬¸í•­ í›„ë³´ëŠ” A/B/C ê³µí†µ êµì§‘í•©ìœ¼ë¡œ ìë™ ì œì•ˆ
    A_cols = [c for c in parts["A"].columns if c!="email"]
    B_cols = [c for c in parts["B"].columns if c!="email"]
    C_cols = [c for c in parts["C"].columns if c!="email"]
    common_auto = sorted(list(set(A_cols) & set(B_cols) & set(C_cols)))
    common_ids = st.multiselect("ê³µí†µë¬¸í•­ ì„ íƒ", common_auto, default=common_auto)

    if len(common_ids) < 5:
        st.info("ê³µí†µë¬¸í•­ì€ ìµœì†Œ 5ê°œ ì´ìƒ ì„ íƒí•˜ì„¸ìš”.")
    else:
        # 1) Scree + Parallel (ì„¸íŠ¸ë³„)
        col1, col2, col3 = st.columns(3)
        for col, sid in zip([col1,col2,col3], ["A","B","C"]):
            with col:
                res = scree_and_parallel(parts[sid][common_ids], n_perm=300, show_plot=True)
                st.pyplot(res['fig'])
                st.caption(f"{sid}: k*={res['k_star']}")

        # 2) Procrustes ì¼ì¹˜ë„
        k_rec = int(np.median([
            scree_and_parallel(parts[s][common_ids], n_perm=300, show_plot=False)['k_star']
            for s in ["A","B","C"]
        ]))
        k_rec = max(2, min(6, k_rec))
        cong = congruence_across_sets(parts["A"], parts["B"], parts["C"], common_ids, k=k_rec)
        st.write(f"ê¶Œê³  ìš”ì¸ ìˆ˜ k={k_rec}")
        st.dataframe(pd.DataFrame({"pair":["A-B","A-C","B-C"],
                                   "phi_mean":[cong['phi_mean_AB'], cong['phi_mean_AC'], cong['phi_mean_BC']]}))
        st.dataframe(pd.DataFrame({"phi_AB":cong['phi_AB'],
                                   "phi_AC":cong['phi_AC'],
                                   "phi_BC":cong['phi_BC']}))

        # 3) ì„¤ëª…ë¶„ì‚°
        st.dataframe(pd.DataFrame(cong['explained_var'], index=["explained_var"]))

# ---------- êµ¬ë³„ì§„ìˆ  ----------
with tabDist:
    st.subheader("êµ¬ë³„ì§„ìˆ (ì •ì˜ì •ë ¬ ê·¼ì‚¬) & Humphreyâ€™s Rule")
    sid = st.selectbox("ì„¸íŠ¸ ì„ íƒ", ["A","B","C"], index=0)
    cols = [c for c in parts[sid].columns if c!="email"]
    use_common = st.checkbox("ê³µí†µë¬¸í•­ë§Œ ì‚¬ìš©(êµì§‘í•©)", value=True)
    if use_common:
        common_auto = sorted(list(set([c for c in parts["A"].columns if c!="email"]) &
                                  set([c for c in parts["B"].columns if c!="email"]) &
                                  set([c for c in parts["C"].columns if c!="email"])))
        target_cols = common_auto
    else:
        target_cols = cols
    if len(target_cols)<5:
        st.info("ë¬¸í•­ì„ 5ê°œ ì´ìƒ í™•ë³´í•˜ì„¸ìš”.")
    else:
        k = st.number_input("ìš”ì¸ ìˆ˜(0=ìë™)", 0, 6, 0, 1)
        k = None if k==0 else int(k)
        Z, Lp = q_factor_solution(parts[sid][target_cols], k=k if k else 5)
        dist = distinguishing_tests(Z, alpha=0.01, se=0.30)
        flags, thr = humphreys_rule(Lp)
        st.markdown(f"Humphreyâ€™s rule ì„ê³„: **{thr:.3f}**")
        st.dataframe(pd.DataFrame({"Factor":list(flags.keys()), "Pass":[int(v) for v in flags.values()]}))
        st.markdown("**z-array (ë¬¸í•­Ã—ìš”ì¸)**")
        st.dataframe(Z)
        st.markdown("**êµ¬ë³„ì§„ìˆ  í›„ë³´(ìœ ì˜)**")
        st.dataframe(dist.head(50))
        st.download_button("ğŸ“¥ êµ¬ë³„ì§„ìˆ  CSV", data=dist.to_csv(index=False).encode("utf-8-sig"),
                           file_name=f"distinguishing_{sid}.csv", mime="text/csv")

# ---------- ë¶€íŠ¸ìŠ¤íŠ¸ë© ----------
with tabBoot:
    st.subheader("ë¶€íŠ¸ìŠ¤íŠ¸ë© ì•ˆì •ë„(ê³µí†µë¬¸í•­)")
    common_auto = sorted(list(set([c for c in parts["A"].columns if c!="email"]) &
                              set([c for c in parts["B"].columns if c!="email"]) &
                              set([c for c in parts["C"].columns if c!="email"])))
    common_ids = st.multiselect("ê³µí†µë¬¸í•­ ì„ íƒ", common_auto, default=common_auto)
    B = st.number_input("ë¶€íŠ¸ìŠ¤íŠ¸ë© ë°˜ë³µ ìˆ˜", 100, 2000, 500, 50)
    phi_thr = st.slider("ì¼ì¹˜ ì„ê³„ Ï†", 0.50, 0.95, 0.80, 0.01)
    sid = st.selectbox("ì„¸íŠ¸ ì„ íƒ", ["A","B","C"], index=0)
    if len(common_ids)<5:
        st.info("ê³µí†µë¬¸í•­ 5ê°œ ì´ìƒ ì„ íƒí•´ ì£¼ì„¸ìš”.")
    else:
        res = bootstrap_factor_stability(parts[sid][common_ids], k=5, B=int(B), phi_threshold=float(phi_thr))
        st.dataframe(pd.DataFrame({"phi_mean":res['phi_mean'], "phi_std":res['phi_std'],
                                   "stability_rate(>=phi_thr)":res['stability_rate']},
                                  index=[f"F{i+1}" for i in range(len(res['phi_mean']))]))
        st.caption("stability_rateëŠ” ë¶€íŠ¸ìŠ¤íŠ¸ë© í‘œë³¸ ì¤‘ Ï†â‰¥ì„ê³„ ë¹„ìœ¨(ìš”ì¸ë³„).")

st.success("ì•± ë¡œë”© ì™„ë£Œ. ì¢Œì¸¡ì—ì„œ ì—‘ì…€ ì—…ë¡œë“œ í›„ ê° íƒ­ì—ì„œ ë¶„ì„ì„ ìˆ˜í–‰í•˜ì„¸ìš”.")
