# -*- coding: utf-8 -*-
"""
Q-Methodology Analysis Engine (Python Twin of R Script)
- Author: Prof. Dr. Songhee Kang
- Reference: Brown, S. R. (1980) & R script 'q_runner_all.R' logic.
- Core Features:
  1. Q-Analysis: PCA/Centroid -> Varimax -> Weighted Factor Arrays (Brown's Formula)
  2. Cross-Set Congruence: Tucker's Phi on Factor Arrays
  3. Bootstrap Stability: Robustness test with Noise Injection
  4. Distinguishing Statements: Z-diff significance test (p < .01, .05)
  5. Humphrey's Rule: Factor significance check
  6. Framing ATT: Non-common item bias check
  7. Demographic Analysis: Factor distribution by user attributes
- Update (2025-11-26): Enforced C01-C35 as common items.
- Update (2025-11-29): Added Demographics Tab & Scree Plot. Korean Interpretations added.
"""

import io
import re
import numpy as np
import pandas as pd
import streamlit as st
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
from scipy.stats import pearsonr, spearmanr, norm as normal_dist
from scipy.linalg import orthogonal_procrustes

# ==========================================
# 1. Configuration & Constants
# ==========================================
st.set_page_config(page_title="Refactored Q-Analysis", layout="wide")

MIN_N_FOR_ANALYSIS = 3
RNG_SEED = 42
rng = np.random.default_rng(RNG_SEED)

# Demographic Value Mappings
DEMO_MAP = {
    "ì„±ë³„": {1: "ë‚¨ì", 2: "ì—¬ì"},
    "ì—°ë ¹": {1: "18~29ì„¸", 2: "30~39ì„¸", 3: "40~49ì„¸", 4: "50~59ì„¸", 5: "60ì„¸ì´ìƒ"},
    "ê±°ì£¼ì§€ì—­": {1: "ì„œìš¸", 2: "ì¸ì²œ/ê²½ê¸°", 3: "ëŒ€ì „/ì¶©ì²­/ì„¸ì¢…", 4: "ê´‘ì£¼/ì „ë¼", 5: "ëŒ€êµ¬/ê²½ë¶", 6: "ë¶€ì‚°/ìš¸ì‚°/ê²½ë‚¨", 7: "ê°•ì›/ì œì£¼"},
    "ê±°ì£¼ê¸°ê°„": {1: "2ë…„ ë¯¸ë§Œ", 2: "2~5ë…„", 3: "5~10ë…„", 4: "10ë…„ ì´ìƒ"},
    "í•™ë ¥": {1: "ì¤‘ì¡¸ì´í•˜", 2: "ê³ ì¡¸", 3: "ì „ë¬¸ëŒ€ì¬í•™ì´ìƒ"},
    "ì§ì—…": {1: "ë†/ì„/ì–´ì—…", 2: "ìì˜ì—…", 3: "íŒë§¤/ì„œë¹„ìŠ¤ì§", 4: "ìƒì‚°/ê¸°ëŠ¥/ë…¸ë¬´", 5: "ì‚¬ë¬´/ê´€ë¦¬/ì „ë¬¸", 6: "ì£¼ë¶€", 7: "í•™ìƒ", 8: "ë¬´ì§/ê¸°íƒ€"},
    "ì¢…ì‚¬ìì—¬ë¶€": {1: "ìˆìŒ", 2: "ì—†ìŒ", 3: "ëª¨ë¦„"}, # ê°€ì¡±, ì§€ì¸ì˜ ì „ë ¥ì‚°ì—… ì¢…ì‚¬ì ì—¬ë¶€
    "ê°€êµ¬ì†Œë“": {1: "200ë§Œì›ë¯¸ë§Œ", 2: "200~400ë§Œì›", 3: "400~600ë§Œì›", 4: "600~800ë§Œì›", 5: "800ë§Œì› ì´ìƒ", 6: "ëª¨ë¦„/ë¬´ì‘ë‹µ"},
    "ê°œì¸ì†Œë“": {1: "200ë§Œì›ë¯¸ë§Œ", 2: "200~400ë§Œì›", 3: "400~600ë§Œì›", 4: "600~800ë§Œì›", 5: "800ë§Œì› ì´ìƒ", 6: "ëª¨ë¦„/ë¬´ì‘ë‹µ"},
    "ì´ë…ì„±í–¥": {1: "ì§„ë³´", 2: "ì¤‘ë„", 3: "ë³´ìˆ˜"}
}

# Helper to find matching key in DEMO_MAP for a given column name
def find_demo_key(col_name):
    for key in DEMO_MAP.keys():
        # Simple keyword matching
        if key in col_name or (key == "ê±°ì£¼ê¸°ê°„" and "ê±°ì£¼" in col_name and "ê¸°ê°„" in col_name):
            return key
        if "ì¢…ì‚¬ì" in col_name and key == "ì¢…ì‚¬ìì—¬ë¶€": return key
        if "ê°€êµ¬" in col_name and "ì†Œë“" in col_name and key == "ê°€êµ¬ì†Œë“": return key
        if "ê°œì¸" in col_name and "ì†Œë“" in col_name and key == "ê°œì¸ì†Œë“": return key
    return None

# ==========================================
# 2. Math & Q-Logic Core (The Engine)
# ==========================================

def standardize_rows(X):
    """Row-wise Z-score normalization"""
    mean = np.nanmean(X, axis=1, keepdims=True)
    std = np.nanstd(X, axis=1, ddof=1, keepdims=True)
    std[std == 0] = 1.0
    return (X - mean) / std

def tuckers_phi(vec_a, vec_b):
    """Tucker's Congruence Coefficient (Phi)"""
    numerator = np.dot(vec_a, vec_b)
    denominator = np.linalg.norm(vec_a) * np.linalg.norm(vec_b)
    if denominator == 0: return 0.0
    return numerator / denominator

class QEngine:
    def __init__(self, data_df, n_factors=3, rotation=True, corr_method='spearman'):
        self.raw_df = data_df
        self.n_factors = n_factors
        self.rotation = rotation
        self.corr_method = corr_method
        
        # Data Cleaning
        temp_data = data_df.apply(pd.to_numeric, errors='coerce').values
        row_means = np.nanmean(temp_data, axis=1)
        inds = np.where(np.isnan(temp_data))
        temp_data[inds] = np.take(row_means, inds[0])
        
        self.data = np.nan_to_num(temp_data, nan=0.0)
        self.n_persons, self.n_items = self.data.shape
        self.loadings = None
        self.factor_arrays = None
        self.explained_variance = None
        self.eigenvalues = None
        
    def fit(self):
        # 1. Correlation Matrix
        if self.corr_method == 'spearman':
            R, _ = spearmanr(self.data, axis=1)
            z_data = standardize_rows(self.data) 
        else:
            z_data = standardize_rows(self.data)
            R = np.corrcoef(z_data)
        R = np.nan_to_num(R, nan=0.0)
        
        # 2. Eigen Decomposition
        eigvals, eigvecs = np.linalg.eigh(R)
        idx = eigvals.argsort()[::-1]
        eigvals = eigvals[idx]
        eigvecs = eigvecs[:, idx]
        self.eigenvalues = eigvals
        
        # 3. Extract Factors
        k = self.n_factors
        valid_eigvals = np.maximum(eigvals[:k], 0)
        L = eigvecs[:, :k] * np.sqrt(valid_eigvals)
        
        # 4. Varimax Rotation
        if self.rotation and k > 1:
            L = self._varimax(L)
            
        self.loadings = L
        self.explained_variance = eigvals[:k]
        
        # 5. Factor Arrays
        self.factor_arrays = self._calculate_factor_arrays(L, z_data)
        return self

    def _varimax(self, Phi, gamma=1.0, q=20, tol=1e-6):
        p, k = Phi.shape
        R = np.eye(k)
        d = 0
        for i in range(q):
            d_old = d
            Lambda = np.dot(Phi, R)
            u, s, vh = np.linalg.svd(
                np.dot(Phi.T, (Lambda**3 - (gamma/p) * np.dot(Lambda, np.diag(np.diag(np.dot(Lambda.T, Lambda))))))
            )
            R = np.dot(u, vh)
            d = np.sum(s)
            if d_old != 0 and d/d_old < 1 + tol: break
        return np.dot(Phi, R)

    def _calculate_factor_arrays(self, loadings, z_data):
        n_items = z_data.shape[1]
        arrays = np.zeros((n_items, self.n_factors))
        for f in range(self.n_factors):
            l_vec = loadings[:, f]
            l_clean = np.clip(l_vec, -0.95, 0.95)
            weights = l_clean / (1 - l_clean**2)
            if np.sum(np.abs(weights)) < 1e-6:
                arrays[:, f] = 0
                continue
            weighted_sum = np.dot(weights, z_data)
            arr_mean = np.mean(weighted_sum)
            arr_std = np.std(weighted_sum, ddof=1)
            if arr_std == 0: arr_std = 1.0
            arrays[:, f] = (weighted_sum - arr_mean) / arr_std
        return arrays

def check_humphreys_rule(loadings, n_persons):
    n_factors = loadings.shape[1]
    threshold = 2 * (1 / np.sqrt(n_persons))
    results = []
    for f in range(n_factors):
        abs_loads = np.sort(np.abs(loadings[:, f]))[::-1]
        if len(abs_loads) >= 2:
            prod = abs_loads[0] * abs_loads[1]
            pass_rule = prod > threshold
            results.append({"Factor": f"F{f+1}", "Product": prod, "Threshold": threshold, "Pass": pass_rule})
        else:
             results.append({"Factor": f"F{f+1}", "Product": 0, "Threshold": threshold, "Pass": False})
    return pd.DataFrame(results)

def find_distinguishing_items_r_logic(factor_arrays, n_factors, item_labels=None, se=0.30, alpha=0.01):
    col_names = [f"F{i+1}" for i in range(n_factors)]
    df_arrays = pd.DataFrame(factor_arrays, columns=col_names)
    if item_labels is not None: df_arrays.index = item_labels
    distinguishing_dict = {}
    crit_z = normal_dist.ppf(1 - alpha/2)

    for i in range(n_factors):
        target_col = f"F{i+1}"
        other_cols = [c for c in df_arrays.columns if c != target_col]
        if not other_cols: continue 
        
        is_higher_all = pd.Series(True, index=df_arrays.index)
        is_lower_all = pd.Series(True, index=df_arrays.index)
        min_diff_val = pd.Series(np.inf, index=df_arrays.index)
        
        for other in other_cols:
            diff = df_arrays[target_col] - df_arrays[other]
            z_stat = diff / (np.sqrt(2) * se)
            is_higher_all &= (z_stat > crit_z)
            is_lower_all &= (z_stat < -crit_z)
            
            current_diff_abs = np.abs(diff)
            update_mask = current_diff_abs < np.abs(min_diff_val)
            min_diff_val[update_mask] = diff[update_mask]

        dist_mask = is_higher_all | is_lower_all
        dist_items = df_arrays[dist_mask].copy()
        
        if not dist_items.empty:
            dist_items['Distinction'] = np.where(is_higher_all[dist_mask], 'Higher', 'Lower')
            dist_items['Min Difference'] = min_diff_val[dist_mask]
            dist_items['Z-Stat'] = dist_items['Min Difference'] / (np.sqrt(2) * se)
            dist_items['P-Value'] = 2 * (1 - normal_dist.cdf(np.abs(dist_items['Z-Stat'])))
            dist_items = dist_items.sort_values('Min Difference', ascending=False, key=abs)
            distinguishing_dict[target_col] = dist_items
    return distinguishing_dict

# ==========================================
# 3. Stability & Congruence Logic
# ==========================================

@st.cache_data(show_spinner=False)
def bootstrap_stability(df_values, n_factors=3, n_boot=200, corr_method='spearman', noise_std=0.0):
    base_engine = QEngine(pd.DataFrame(df_values), n_factors=n_factors, corr_method=corr_method).fit()
    base_arrays = base_engine.factor_arrays 
    n_persons = df_values.shape[0]
    phi_results = np.zeros((n_boot, n_factors))
    
    for b in range(n_boot):
        indices = rng.choice(n_persons, size=n_persons, replace=True)
        if len(np.unique(indices)) < 3:
            phi_results[b, :] = np.nan
            continue
        sample_data = pd.DataFrame(df_values[indices])
        if noise_std > 0:
            sample_data += rng.normal(0, noise_std, sample_data.shape)
        
        boot_engine = QEngine(sample_data, n_factors=n_factors, corr_method=corr_method).fit()
        boot_arrays = boot_engine.factor_arrays
        
        for f in range(n_factors):
            target = base_arrays[:, f]
            best_phi = -1.0
            for bf in range(n_factors):
                phi = tuckers_phi(target, boot_arrays[:, bf])
                if abs(phi) > abs(best_phi): best_phi = phi
            phi_results[b, f] = abs(best_phi)
    
    phi_results = phi_results[~np.isnan(phi_results).any(axis=1)]
    return {
        "mean": np.mean(phi_results, axis=0),
        "std": np.std(phi_results, axis=0),
        "rate_90": np.mean(phi_results >= 0.90, axis=0)
    }

@st.cache_data(show_spinner=False)
def calculate_cross_set_congruence(parts_q, common_cols, n_factors=3, corr_method='spearman'):
    engines = {}
    for name, df in parts_q.items():
        df_common = df[common_cols]
        engine = QEngine(df_common, n_factors=n_factors, corr_method=corr_method).fit()
        engines[name] = engine.factor_arrays 
    results = []
    pairs = [('A','B'), ('A','C'), ('B','C')]
    for s1, s2 in pairs:
        if s1 not in engines or s2 not in engines: continue
        arr1, arr2 = engines[s1], engines[s2]
        phis = [abs(tuckers_phi(arr1[:,f], arr2[:,f])) for f in range(n_factors)]
        results.append({"Pair": f"{s1}-{s2}", "Mean Phi": np.mean(phis), "Factors": phis})
    return results

# ==========================================
# 4. Data Loading & UI
# ==========================================

def parse_uploaded_file(file):
    xls = pd.ExcelFile(file)
    valid_names = ["PARTA", "PARTB", "PARTC"]
    
    # Store Q-data and Demo-data separately
    q_parts = {}
    d_parts = {}
    
    # Metadata keywords to exclude
    meta_keywords = ['time', 'date', 'duration', 'ip', 'token']
    # Q-item pattern (C01, C02...)
    q_pattern = re.compile(r"^C(0[1-9]|[12][0-9]|3[0-9]|4[0-9])$", re.IGNORECASE)

    for sname in valid_names:
        if sname in xls.sheet_names:
            df = pd.read_excel(xls, sname)
            # Standardize ID column
            id_col = next((c for c in df.columns if str(c).lower() in ['email', 'id', 'respondent', 'pid']), None)
            if not id_col:
                df['ID'] = [f"P{i+1}" for i in range(len(df))]
                id_col = 'ID'
            else:
                df[id_col] = df[id_col].astype(str)
            
            df = df.set_index(id_col)
            numeric_df = df.apply(pd.to_numeric, errors='coerce')

            # Split Columns
            q_cols = []
            d_cols = []
            
            for c in df.columns:
                if q_pattern.match(str(c)):
                    q_cols.append(c)
                elif c not in [id_col] and not any(k in str(c).lower() for k in meta_keywords):
                    # Potential demographic column if numeric and not metadata
                    # Check if it has numeric content (not all NaNs)
                    if numeric_df[c].notna().sum() > 0:
                        d_cols.append(c)

            # Clean Q-Data
            if q_cols:
                q_df = numeric_df[q_cols].dropna(thresh=len(q_cols)*0.8)
                q_parts[sname.replace("PART", "")] = q_df
                
                # Keep corresponding Demographics for the valid Q-sort rows
                d_df = df.loc[q_df.index, d_cols]
                d_parts[sname.replace("PART", "")] = d_df
                
    return q_parts, d_parts

def get_common_columns(parts):
    # Strict Regex C01-C35
    pat = re.compile(r"^C(0[1-9]|[12][0-9]|3[0-5])$", re.IGNORECASE)
    sets_cols = []
    for df in parts.values():
        cols = {c for c in df.columns if pat.match(str(c))}
        sets_cols.append(cols)
    if not sets_cols: return []
    return sorted(list(set.intersection(*sets_cols)))

def calculate_framing_att(parts, common_cols):
    summary = []
    for name, df in parts.items():
        non_common = [c for c in df.columns if c not in common_cols]
        if not non_common:
            mean_val, n_items, items_str = 0, 0, "-"
        else:
            mean_val = np.nanmean(df[non_common].values)
            n_items = len(non_common)
            items_str = f"{non_common[0]}...{non_common[-1]} ({len(non_common)})" if len(non_common)>5 else ", ".join(non_common)
        summary.append({"Set": name, "Non-Common Mean": mean_val, "N_Items": n_items, "Items": items_str})
    return pd.DataFrame(summary).set_index("Set")

# ==========================================
# 5. Main Execution
# ==========================================

st.title("Q-ë°©ë²•ë¡  ë¶„ì„ ì—”ì§„ v2.0")
st.markdown("""
> **ì—…ë°ì´íŠ¸ (2025-11-29):**
> 1. **ì¸êµ¬í†µê³„í•™ì  ë¶„ì„ (Tab 7):** ì„±ë³„, ì—°ë ¹, ì†Œë“ ë“± ì£¼ìš” ë³€ìˆ˜ë³„ ìš”ì¸ ë¶„í¬ ë¶„ì„ ê¸°ëŠ¥ ì¶”ê°€.
> 2. **ìŠ¤í¬ë¦¬ ë„í‘œ (Scree Plot):** ê³ ìœ ê°’ ë³€í™” ê·¸ë˜í”„ ì¶”ê°€ (Tab 1).
> 3. **í•œê¸€ í•´ì„ ê°•í™”:** ê° ë¶„ì„ íƒ­ì— ìƒì„¸í•œ í•œê¸€ ì„¤ëª… ë°•ìŠ¤(Interpretation) ì¶”ê°€.
""")

uploaded_file = st.sidebar.file_uploader("Upload Excel (PARTA/B/C)", type='xlsx')

if uploaded_file:
    q_parts, d_parts = parse_uploaded_file(uploaded_file)
    
    if not q_parts:
        st.error("ë°ì´í„°ë¥¼ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‹œíŠ¸ëª…(PARTA...)ê³¼ ë¬¸í•­ ì½”ë“œ(C01...)ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
    else:
        st.sidebar.success(f"Loaded Sets: {list(q_parts.keys())}")
        common_cols = get_common_columns(q_parts)
        
        tab1, tab2, tab3, tab4, tab5, tab6, tab7 = st.tabs([
            "1. Basic Q-Analysis", 
            "2. Humphrey's Rule",
            "3. Distinguishing Items",
            "4. Cross-Set Congruence", 
            "5. Bootstrap Stability", 
            "6. Framing ATT",
            "7. Demographic Analysis"
        ])
        
        # Shared State
        target_set = st.sidebar.selectbox("Target Set for Tabs 1,2,3,5,7", list(q_parts.keys()))
        n_factors = st.sidebar.number_input("Number of Factors", 1, 7, 3)
        corr_method = st.sidebar.selectbox("Correlation Method", ["pearson", "spearman"], index=1)
        
        # Run Engine for Target Set
        df = q_parts[target_set]
        engine = QEngine(df, n_factors=n_factors, corr_method=corr_method).fit()

        # --- Tab 1: Basic Analysis ---
        with tab1:
            st.header("Basic Q-Analysis Result")
            st.info("ğŸ’¡ **í•´ì„(Interpretation):**\n- **Explained Variance:** ê° ìš”ì¸ì´ ì „ì²´ ë°ì´í„°ì˜ ë³€ë™ì„±ì„ ì–¼ë§ˆë‚˜ ì„¤ëª…í•˜ëŠ”ì§€ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. (ë³´í†µ 40% ì´ìƒì´ë©´ ì–‘í˜¸)\n- **Scree Plot:** ê³ ìœ ê°’ì´ ê¸‰ê²©íˆ ë–¨ì–´ì§€ë‹¤ê°€ ì™„ë§Œí•´ì§€ëŠ” ì§€ì (Elbow) ì „ê¹Œì§€ë¥¼ ì˜ë¯¸ ìˆëŠ” ìš”ì¸ ìˆ˜ë¡œ ë´…ë‹ˆë‹¤.")
            
            col1, col2 = st.columns(2)
            with col1:
                st.subheader("Explained Variance")
                st.dataframe(pd.DataFrame(engine.explained_variance, index=[f"F{i+1}" for i in range(n_factors)], columns=["Eigenvalue"]).T)
                
                # Scree Plot
                st.subheader("Scree Plot")
                fig, ax = plt.subplots(figsize=(6, 4))
                ax.plot(range(1, len(engine.eigenvalues)+1), engine.eigenvalues, 'bo-', markersize=6)
                ax.axhline(y=1.0, color='r', linestyle='--', linewidth=0.8, label='Eigenvalue=1')
                ax.set_title("Scree Plot (Eigenvalues)")
                ax.set_xlabel("Factor Number")
                ax.set_ylabel("Eigenvalue")
                ax.legend()
                ax.grid(True, linestyle=':', alpha=0.6)
                st.pyplot(fig)
                
            with col2:
                st.subheader("Factor Distribution")
                # Factor Assignment (Highest Loading)
                max_vals = np.max(np.abs(engine.loadings), axis=1)
                max_idxs = np.argmax(np.abs(engine.loadings), axis=1)
                valid_types = [f"Type {i+1}" if v > 0.4 else "None" for i, v in zip(max_idxs, max_vals)]
                
                s_counts = pd.Series(valid_types).value_counts().sort_index()
                st.bar_chart(s_counts)
                st.caption(f"Total Participants: {len(df)}")

            st.subheader("Factor Loadings (Rotated)")
            st.dataframe(pd.DataFrame(engine.loadings, index=df.index, columns=[f"F{i+1}" for i in range(n_factors)]).style.background_gradient(cmap="Blues"))

        # --- Tab 2: Humphrey's Rule ---
        with tab2:
            st.header("Humphrey's Rule Validation")
            st.info("ğŸ’¡ **í•´ì„(Interpretation):**\n- í—˜í”„ë¦¬ ë²•ì¹™ì€ ìš”ì¸ì˜ í†µê³„ì  ìœ ì˜ì„±ì„ ê²€ì¦í•©ë‹ˆë‹¤.\n- ë‘ ê°œì˜ ê°€ì¥ ë†’ì€ ì ì¬ëŸ‰ì˜ ê³±ì´ í‘œì¤€ì˜¤ì°¨ì˜ 2ë°°ë¥¼ ë„˜ì–´ì•¼ ìœ ì˜ë¯¸í•œ ìš”ì¸ìœ¼ë¡œ ê°„ì£¼í•©ë‹ˆë‹¤.\n- 'Pass'ê°€ Trueì¸ ìš”ì¸ë§Œ í•´ì„í•˜ëŠ” ê²ƒì´ ì•ˆì „í•©ë‹ˆë‹¤.")
            
            res_hum = check_humphreys_rule(engine.loadings, engine.n_persons)
            st.dataframe(res_hum.style.applymap(lambda x: 'color: green; font-weight: bold' if x else 'color: red', subset=['Pass']))

        # --- Tab 3: Distinguishing Items ---
        with tab3:
            st.header("Distinguishing Statements")
            st.info("ğŸ’¡ **í•´ì„(Interpretation):**\n- íŠ¹ì • ìš”ì¸ì´ ë‹¤ë¥¸ ëª¨ë“  ìš”ì¸ê³¼ í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•˜ê²Œ(p<.01 or .05) ì°¨ì´ë‚˜ëŠ” ë¬¸í•­ë“¤ì…ë‹ˆë‹¤.\n- ì´ ë¬¸í•­ë“¤ì€ í•´ë‹¹ ìœ í˜•(Type)ì˜ ë…íŠ¹í•œ íŠ¹ì„±ì„ ì„¤ëª…í•˜ëŠ” í•µì‹¬ ë‹¨ì„œê°€ ë©ë‹ˆë‹¤.")
            
            c1, c2 = st.columns(2)
            with c1: alpha_level = st.selectbox("Alpha Level", [0.01, 0.05], index=0)
            with c2: se_val = st.number_input("Standard Error", 0.1, 1.0, 0.30, step=0.01)

            dist_dict = find_distinguishing_items_r_logic(engine.factor_arrays, n_factors, item_labels=df.columns, se=se_val, alpha=alpha_level)
            
            subtabs = st.tabs([f"Type {i+1}" for i in range(n_factors)])
            for i, stab in enumerate(subtabs):
                with stab:
                    f_key = f"F{i+1}"
                    items_df = dist_dict.get(f_key)
                    if items_df is not None:
                        st.write(f"**Found {len(items_df)} distinguishing items (p < {alpha_level})**")
                        st.dataframe(items_df.style.background_gradient(cmap="coolwarm", subset=["Min Difference"], vmin=-2, vmax=2))
                    else:
                        st.warning("No distinguishing items found for this factor.")

        # --- Tab 4: Cross-Set Congruence ---
        with tab4:
            st.header("Cross-Set Congruence")
            st.info("ğŸ’¡ **í•´ì„(Interpretation):**\n- Tucker's Phi ê³„ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì„œë¡œ ë‹¤ë¥¸ ë°ì´í„°ì…‹(PART A, B, C)ì—ì„œ ë„ì¶œëœ ìš”ì¸ë“¤ì´ ì–¼ë§ˆë‚˜ ìœ ì‚¬í•œì§€ ë¹„êµí•©ë‹ˆë‹¤.\n- **0.90 ì´ìƒ:** ë§¤ìš° ë†’ì€ ìœ ì‚¬ì„± (ë™ì¼ ìš”ì¸ìœ¼ë¡œ ê°„ì£¼ ê°€ëŠ¥)\n- **0.80 ~ 0.90:** ë†’ì€ ìœ ì‚¬ì„±")
            
            if len(common_cols) < 5:
                st.warning("ê³µí†µ ë¬¸í•­(C01~C35)ì´ ì¶©ë¶„í•˜ì§€ ì•Šì•„ êµì°¨ ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            else:
                results = calculate_cross_set_congruence(q_parts, common_cols, n_factors, corr_method)
                for res in results:
                    st.subheader(f"Comparison: {res['Pair']}")
                    cols = st.columns(len(res['Factors']))
                    for i, phi in enumerate(res['Factors']):
                        cols[i].metric(f"Factor {i+1}", f"{phi:.3f}", delta_color="normal" if phi>0.9 else "off")
                    st.divider()

        # --- Tab 5: Stability ---
        with tab5:
            st.header("Bootstrap Stability")
            st.info("ğŸ’¡ **í•´ì„(Interpretation):**\n- ë°ì´í„°ë¥¼ ë¬´ì‘ìœ„ë¡œ ë³µì› ì¶”ì¶œ(Resampling)í•˜ì—¬ ìš”ì¸ êµ¬ì¡°ê°€ ì–¼ë§ˆë‚˜ ì•ˆì •ì ì¸ì§€ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.\n- **Rate > 0.90**ì€ ìš”ì¸ êµ¬ì¡°ê°€ ë§¤ìš° ê²¬ê³ í•¨ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.")
            
            if st.button("Run Bootstrap Analysis"):
                with st.spinner("Running 100 iterations..."):
                    res = bootstrap_stability(df.values, n_factors=3, n_boot=100, corr_method=corr_method, noise_std=0.05)
                st.dataframe(pd.DataFrame(res, index=[f"F{i+1}" for i in range(len(res['mean']))]).style.background_gradient(cmap="Greens", subset=['mean']))

        # --- Tab 6: Framing ATT ---
        with tab6:
            st.header("Framing ATT (Non-Common Items)")
            st.info("ğŸ’¡ **í•´ì„(Interpretation):**\n- ê³µí†µ ë¬¸í•­(Common Items)ì„ ì œì™¸í•œ ë¹„ê³µí†µ ë¬¸í•­(Unique Items)ë“¤ì˜ í‰ê·  ì ìˆ˜ ì°¨ì´ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.\n- ì°¨ì´ê°€ í´ìˆ˜ë¡ ë¬¸í•­ êµ¬ì„±(Framing)ì— ë”°ë¥¸ ì‘ë‹µ í¸í–¥ì´ ì¡´ì¬í•  ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤.")
            
            att_df = calculate_framing_att(q_parts, common_cols)
            st.dataframe(att_df)
            
            if len(att_df) >= 2:
                st.markdown("#### Pairwise Differences")
                sets = att_df.index.tolist()
                pairs = [(a, b) for idx, a in enumerate(sets) for b in sets[idx+1:]]
                diffs = [{"Pair": f"{s2} - {s1}", "Difference": att_df.loc[s2, "Non-Common Mean"] - att_df.loc[s1, "Non-Common Mean"]} for s1, s2 in pairs]
                st.dataframe(pd.DataFrame(diffs))

        # --- Tab 7: Demographic Analysis (NEW) ---
        with tab7:
            st.header("Demographic Analysis")
            st.info("ğŸ’¡ **í•´ì„(Interpretation):**\n- ê° ìš”ì¸(Type)ì— ì†í•œ ì‚¬ëŒë“¤ì˜ ì¸êµ¬í†µê³„í•™ì  íŠ¹ì„±(ì„±ë³„, ì—°ë ¹, ì§ì—… ë“±)ì„ ë¶„ì„í•©ë‹ˆë‹¤.\n- ì œê³µëœ ì½”ë”©ê°’(1, 2, 3...)ì€ ìë™ìœ¼ë¡œ ë¼ë²¨(ë‚¨ì, ì—¬ì...)ë¡œ ë³€í™˜ë©ë‹ˆë‹¤.")
            
            # 1. Assign Factors to Persons
            max_idxs = np.argmax(np.abs(engine.loadings), axis=1)
            # Only consider valid if loading > 0.4 (Optional, but usually good practice. Here we classify everyone for demo analysis)
            # If strictly needed, filter: factor_labels = [f"Type {i+1}" if abs(engine.loadings[r, i]) > 0.4 else "None" for r, i in enumerate(max_idxs)]
            factor_labels = [f"Type {i+1}" for i in max_idxs]
            
            # 2. Merge with Demographics
            if target_set in d_parts:
                demo_df = d_parts[target_set].copy()
                # Ensure indices match
                common_indices = demo_df.index.intersection(df.index)
                if len(common_indices) == 0:
                    st.error("Q-sort ë°ì´í„°ì™€ ì¸êµ¬í†µê³„ ë°ì´í„°ì˜ ID(PID)ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                else:
                    demo_subset = demo_df.loc[common_indices]
                    # Add Factor Column
                    # Use df.index.get_indexer to map factors correctly
                    factor_series = pd.Series(factor_labels, index=df.index)
                    demo_subset['Assigned_Factor'] = factor_series.loc[common_indices]
                    
                    st.write(f"**Analyzed Participants:** {len(demo_subset)} ëª…")
                    
                    # 3. Iterate Analysis
                    found_demo_cols = False
                    for col in demo_subset.columns:
                        if col == 'Assigned_Factor': continue
                        
                        # Find mapping key
                        map_key = find_demo_key(str(col))
                        
                        if map_key:
                            found_demo_cols = True
                            st.markdown(f"### {col} ({map_key})")
                            
                            # Apply Mapping
                            mapped_col = demo_subset[col].map(DEMO_MAP[map_key])
                            # Handle unmapped values (keep original or mark unknown)
                            mapped_col = mapped_col.fillna("Unknown/Other")
                            
                            # Cross-tabulation
                            ct = pd.crosstab(mapped_col, demo_subset['Assigned_Factor'])
                            
                            # Display Side-by-Side
                            c1, c2 = st.columns([1, 2])
                            with c1:
                                st.dataframe(ct)
                            with c2:
                                st.bar_chart(ct)
                            st.divider()
                    
                    if not found_demo_cols:
                        st.warning("ì§€ì •ëœ ì¸êµ¬í†µê³„ ì»¬ëŸ¼(ì„±ë³„, ì—°ë ¹ ë“±)ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì—‘ì…€ íŒŒì¼ì˜ ì»¬ëŸ¼ëª…ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
                        st.dataframe(demo_subset.head())
            else:
                st.warning("ì´ ë°ì´í„°ì…‹(Set)ì—ëŠ” ì¸êµ¬í†µê³„ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")

else:
    st.info("ë¶„ì„í•  ì—‘ì…€ íŒŒì¼(PARTA, PARTB, PARTC ì‹œíŠ¸ í¬í•¨)ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
